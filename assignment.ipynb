{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63395054",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from collections import deque\n",
    "import pprint\n",
    "import os # 拡張子をチェックするために os.path を使います\n",
    "\n",
    "# --- 設定 ---\n",
    "\n",
    "# 1. 開始URL\n",
    "START_URL = \"https://www.musashino-u.ac.jp/\"\n",
    "\n",
    "# 2. クロール対象のドメイン\n",
    "BASE_DOMAIN = \"musashino-u.ac.jp\"\n",
    "\n",
    "# 3. 負荷軽減のための待機時間（秒）\n",
    "WAIT_TIME = 1\n",
    "\n",
    "# 4. クロールする最大ページ数（サイト全体をクロールすると時間がかかりすぎるため）\n",
    "MAX_PAGES = 50 \n",
    "\n",
    "# 5. 【NEW】クロール対象から除外するファイルの拡張子\n",
    "#    これらに該当するURLはたどりません。\n",
    "EXCLUDE_EXTENSIONS = [\n",
    "    # 画像\n",
    "    '.png', '.jpg', '.jpeg', '.gif', '.svg', '.bmp', '.webp',\n",
    "    # ドキュメント\n",
    "    '.pdf', '.doc', '.docx', '.xls', '.xlsx', '.ppt', '.pptx',\n",
    "    # 圧縮ファイル\n",
    "    '.zip', '.rar', '.gz',\n",
    "    # メディア\n",
    "    '.mp4', '.mov', '.avi', '.mp3', '.wav',\n",
    "    # その他\n",
    "    '.css', '.js'\n",
    "]\n",
    "\n",
    "# ----------------\n",
    "\n",
    "def crawl_sitemap_v2():\n",
    "    \"\"\"\n",
    "    指定されたURLからクロールを開始し、サイトマップ（URL: title）の辞書を作成する。\n",
    "    画像やPDFなどのファイルへのリンクは除外する。\n",
    "    \"\"\"\n",
    "    \n",
    "    # サイトマップを格納する辞書\n",
    "    sitemap = {}\n",
    "    \n",
    "    # これから訪問するURLを管理するキュー\n",
    "    queue = deque([START_URL])\n",
    "    \n",
    "    # 既に訪問した、またはキューに追加したURLを管理するセット\n",
    "    visited_urls = {START_URL}\n",
    "    \n",
    "    # リクエスト時のヘッダー\n",
    "    headers = {\n",
    "        \"User-Agent\": \"MySitemapScraper/1.0 (Contact: your-email@example.com)\"\n",
    "    }\n",
    "\n",
    "    print(f\"クロールを開始します... (最大{MAX_PAGES}ページ)\")\n",
    "    print(f\"除外する拡張子: {EXCLUDE_EXTENSIONS}\")\n",
    "\n",
    "    while queue and len(sitemap) < MAX_PAGES:\n",
    "        # キューからURLを1つ取り出す\n",
    "        current_url = queue.popleft()\n",
    "\n",
    "        print(f\"[{len(sitemap) + 1}/{MAX_PAGES}] 処理中: {current_url}\", end=\"\")\n",
    "\n",
    "        try:\n",
    "            # --- 課題要件: 負荷軽減 ---\n",
    "            time.sleep(WAIT_TIME)\n",
    "\n",
    "            # --- 課題要件: ページにアクセス ---\n",
    "            response = requests.get(current_url, headers=headers, timeout=10)\n",
    "            \n",
    "            if response.status_code != 200:\n",
    "                print(\" -> スキップ (Status Code)\")\n",
    "                continue\n",
    "            \n",
    "            # コンテンツタイプがHTML/テキストでない場合はスキップ\n",
    "            # (万が一、拡張子チェックをすり抜けてもここで弾く)\n",
    "            if 'text/html' not in response.headers.get('Content-Type', ''):\n",
    "                print(\" -> スキップ (HTMLではありません)\")\n",
    "                continue\n",
    "                \n",
    "            response.encoding = response.apparent_encoding\n",
    "            \n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "            # --- 課題要件: <title> を取得 ---\n",
    "            title_tag = soup.find(\"title\")\n",
    "            title = \"タイトルなし\"\n",
    "            if title_tag and title_tag.string:\n",
    "                title = title_tag.string.strip()\n",
    "            \n",
    "            # --- 課題要件: 辞書型変数に格納 ---\n",
    "            sitemap[current_url] = title\n",
    "            print(f\" -> {title}\")\n",
    "\n",
    "            # --- 課題要件: 同一ドメインの全リンクを巡る ---\n",
    "            links = soup.find_all(\"a\", href=True)\n",
    "\n",
    "            for link in links:\n",
    "                href = link.get(\"href\")\n",
    "\n",
    "                # 1. 相対URLを絶対URLに変換\n",
    "                absolute_url = urljoin(current_url, href)\n",
    "                \n",
    "                # 2. URLを正規化\n",
    "                parsed_url = urlparse(absolute_url)\n",
    "                clean_url = parsed_url._replace(query=\"\", fragment=\"\").geturl()\n",
    "\n",
    "                # 3. URLがクロール対象かチェック\n",
    "                \n",
    "                # (a) パスから拡張子を取得 (例: /foo/bar.PDF -> .pdf)\n",
    "                path = parsed_url.path\n",
    "                # os.path.splitext で拡張子を分離し、小文字に変換\n",
    "                ext = os.path.splitext(path)[1].lower()\n",
    "\n",
    "                # (b) チェックロジック\n",
    "                is_valid = (\n",
    "                    # 同一ドメインか？\n",
    "                    BASE_DOMAIN in parsed_url.netloc and\n",
    "                    # HTTP/HTTPSプロトコルか？\n",
    "                    parsed_url.scheme in [\"http\", \"https\"] and\n",
    "                    # まだ訪問済みリストにないか？\n",
    "                    clean_url not in visited_urls and\n",
    "                    # 【NEW】除外リストにある拡張子ではないか？\n",
    "                    ext not in EXCLUDE_EXTENSIONS and\n",
    "                    # そもそもパスが空（例: \"https://.../\"）の場合もOK\n",
    "                    ext == '' \n",
    "                )\n",
    "\n",
    "                if is_valid:\n",
    "                    # 訪問済みリストとキューに追加\n",
    "                    visited_urls.add(clean_url)\n",
    "                    queue.append(clean_url)\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\" -> エラー ( {e} )\")\n",
    "        except Exception as e:\n",
    "            print(f\" -> 不明なエラー ( {e} )\")\n",
    "\n",
    "    print(\"\\nクロールが完了しました。\")\n",
    "    return sitemap\n",
    "\n",
    "\n",
    "# --- メイン処理 ---\n",
    "if __name__ == \"__main__\":\n",
    "    # クローラーを実行\n",
    "    site_map_data = crawl_sitemap_v2()\n",
    "    \n",
    "    # --- 課題要件: 辞書型変数を print() で表示する ---\n",
    "    print(\"\\n--- サイトマップ（辞書型） ---\")\n",
    "    pprint.pprint(site_map_data)\n",
    "    print(f\"\\n合計 {len(site_map_data)} ページを検出しました。\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
